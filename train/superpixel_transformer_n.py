from timm.models.vision_transformer import VisionTransformer
import timm.models.vision_transformer
import skimage.io as io
import argparse
import joblib
import copy
import random
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import skimage.io as io
from timm.models.layers import drop_path, to_2tuple, trunc_normal_,PatchEmbed
from timm.models.helpers import build_model_with_cfg, named_apply
from torch_geometric.nn import global_mean_pool,global_max_pool,GlobalAttention,dense_diff_pool,global_add_pool,TopKPooling,ASAPooling,SAGPooling
from torch_geometric.nn import GCNConv,ChebConv,SAGEConv,GraphConv,LEConv,LayerNorm,GATConv
import torch
from sklearn.metrics import accuracy_score,f1_score,roc_auc_score
import torch.nn as nn
torch.set_num_threads(8)
import numpy as np
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
from functools import partial
from block_utils import Block
from torch_geometric.data import Data as geomData
from timm.models.layers import trunc_normal_
from torch_scatter import scatter_add
from torch_geometric.utils import softmax
try:
    from torch import _assert
except ImportError:
    def _assert(condition: bool, message: str):
        assert condition, message


def reset(nn):
    def _reset(item):
        if hasattr(item, 'reset_parameters'):
            item.reset_parameters()

    if nn is not None:
        if hasattr(nn, 'children') and len(list(nn.children())) > 0:
            for item in nn.children():
                _reset(item)
        else:
            _reset(nn)

class my_GlobalAttention(torch.nn.Module):
    def __init__(self, gate_nn, nn=None):
        super(my_GlobalAttention, self).__init__()
        self.gate_nn = gate_nn
        self.nn = nn

        self.reset_parameters()

    def reset_parameters(self):
        reset(self.gate_nn)
        reset(self.nn)


    def forward(self, x, batch, size=None):
        """"""
        x = x.unsqueeze(-1) if x.dim() == 1 else x
        size = batch.max().item() + 1 if size is None else size   #modified
        
        gate = self.gate_nn(x).view(-1, 1)
        x = self.nn(x) if self.nn is not None else x
        assert gate.dim() == x.dim() and gate.size(0) == x.size(0)

        gate = softmax(gate, batch, num_nodes=size)   
        out = scatter_add(gate * x, batch, dim=0, dim_size=size)

        return out


class Intra_GCN(nn.Module):
    def __init__(self,in_feats,n_hidden,out_feats,drop_out_ratio=0.2,mpool_method="global_mean_pool",gnn_method='sage'):
        super(Intra_GCN,self).__init__() 
        if gnn_method == 'sage':       
            self.conv1= SAGEConv(in_channels=in_feats,out_channels=out_feats)  
        elif gnn_method == 'gcn':
            self.conv1= GCNConv(in_channels=in_feats,out_channels=out_feats)  
        elif gnn_method == 'gat':
            self.conv1= GATConv(in_channels=in_feats,out_channels=out_feats)  
        elif gnn_method == 'leconv':
            self.conv1= LEConv(in_channels=in_feats,out_channels=out_feats)  
        elif gnn_method == 'graphconv':
            self.conv1= GraphConv(in_channels=in_feats,out_channels=out_feats)  
        # self.conv2= SAGEConv(in_channels=n_hidden,out_channels=out_feats)
        
        self.relu = torch.nn.ReLU() 
        self.sigmoid = torch.nn.Sigmoid()
        self.dropout=nn.Dropout(p=drop_out_ratio)
        self.softmax = nn.Softmax(dim=-1)
        
        if mpool_method == "global_mean_pool":
            self.mpool = global_mean_pool 
        elif mpool_method == "global_max_pool":
            self.mpool = global_max_pool 
        elif mpool_method == "global_att_pool":
            att_net=nn.Sequential(nn.Linear(out_feats, out_feats//2), nn.ReLU(), nn.Linear(out_feats//2, 1))     
            self.mpool = my_GlobalAttention(att_net)  
        self.norm = LayerNorm(in_feats)      
        self.norm2 = LayerNorm(out_feats)
        self.norm1 = LayerNorm(n_hidden)
        
    def forward(self,data):
        x=data.x
        edge_index = data.edge_patch

        x = self.norm(x)
        x = self.conv1(x,edge_index)
        x = self.relu(x)  
        # x = self.sigmoid(x)
        # x = self.norm(x)
        x = self.norm1(x)
        x = self.dropout(x)

        # x = self.conv2(x,edge_index)
        # x = self.relu(x)  
        # # x = self.sigmoid(x)
        # # x = self.norm(x)   
        # x = self.norm2(x)    
        # x = self.dropout(x)
        # print(x)

#         batch = torch.zeros(len(x),dtype=torch.long).to(device)
        batch = data.superpixel_attri.to(device)
        x = self.mpool(x,batch)
        # print('fea dim is {}'.format(x.shape))
        # print(x)
        
        fea = x
        # print(fea.shape)

        return fea

class Inter_GCN(nn.Module):
    def __init__(self,in_feats,n_hidden,out_feats,drop_out_ratio=0.2,mpool_method="global_mean_pool",gnn_method='sage'):
        super(Inter_GCN,self).__init__()        
        # self.conv1= SAGEConv(in_channels=in_feats,out_channels=out_feats)          
        # self.conv2= SAGEConv(in_channels=n_hidden,out_channels=out_feats)
        if gnn_method == 'sage':       
            self.conv1= SAGEConv(in_channels=in_feats,out_channels=out_feats)  
        elif gnn_method == 'gcn':
            self.conv1= GCNConv(in_channels=in_feats,out_channels=out_feats)  
        elif gnn_method == 'gat':
            self.conv1= GATConv(in_channels=in_feats,out_channels=out_feats)  
        elif gnn_method == 'leconv':
            self.conv1= LEConv(in_channels=in_feats,out_channels=out_feats)  
        elif gnn_method == 'graphconv':
            self.conv1= GraphConv(in_channels=in_feats,out_channels=out_feats)  
        
        self.relu = torch.nn.ReLU() 
        self.sigmoid = torch.nn.Sigmoid()
        self.dropout=nn.Dropout(p=drop_out_ratio)
        self.softmax = nn.Softmax(dim=-1)
        
        if mpool_method == "global_mean_pool":
            self.mpool = global_mean_pool 
        elif mpool_method == "global_max_pool":
            self.mpool = global_max_pool 
        elif mpool_method == "global_att_pool":
            att_net=nn.Sequential(nn.Linear(out_feats, out_feats//2), nn.ReLU(), nn.Linear(out_feats//2, 1))     
            self.mpool = my_GlobalAttention(att_net)        
        self.norm = LayerNorm(in_feats)
        self.norm2 = LayerNorm(out_feats)
        self.norm1 = LayerNorm(n_hidden)
        
    def forward(self,data,feature):
        x=feature
        edge_index = data.edge_superpixel
        # print(x.shape)
        x = self.norm(x)
        x = self.conv1(x,edge_index)
        x = self.relu(x)  
        # x = self.sigmoid(x)
        x = self.norm1(x)
        x = self.dropout(x)

        # x = self.conv2(x,edge_index)
        # x = self.relu(x)  
        # # x = self.sigmoid(x)
        # x = self.norm2(x)       
        # x = self.dropout(x)

        # batch = torch.zeros(len(x),dtype=torch.long).to(device)
        # x = self.mpool(x,batch)
        
        fea = x
        # print(x.shape)
        # print(fea.shape)

        return fea



class VisionTransformer(timm.models.vision_transformer.VisionTransformer):
    """ Vision Transformer with support for global average pooling
    """
    def __init__(self, num_patches=100,no_embed_class=False,class_token=True, depth=1,drop_path_rate=0.,mlp_ratio=4.,pre_norm=True,qkv_bias=True,init_values=None,drop_rate=0.,attn_drop_rate=0.,norm_layer=None,act_layer=None,weight_init='',global_pool='token', fc_norm=None,**kwargs):
        super(VisionTransformer, self).__init__(**kwargs)
        embed_dim = kwargs['embed_dim']
        self.patch_embed = nn.Linear(embed_dim,embed_dim)
        num_patches = num_patches
        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)
        act_layer = act_layer or nn.GELU
        self.no_embed_class = no_embed_class
        self.global_pool = global_pool

        self.num_prefix_tokens = 1 if class_token else 0
        embed_len = num_patches if no_embed_class else num_patches + self.num_prefix_tokens
        self.pos_embed = nn.Parameter(torch.randn(1, embed_len, embed_dim) * .02)
        self.norm_pre = norm_layer(embed_dim) if pre_norm else nn.Identity()
        use_fc_norm = global_pool == 'avg' if fc_norm is None else fc_norm
        self.fc_norm = norm_layer(embed_dim) if use_fc_norm else nn.Identity()
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)] 
        
        self.blocks = nn.Sequential(*[
            Block(
                dim=embed_dim,
                num_heads=kwargs['num_heads'],
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                init_values=init_values,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[i],
                norm_layer=norm_layer,
                act_layer=act_layer
            )
            for i in range(depth)])
        self.init_weights(weight_init)

    def init_weights(self, mode=''):
        assert mode in ('jax', 'jax_nlhb', 'moco', '')
        head_bias = -math.log(self.num_classes) if 'nlhb' in mode else 0.
        trunc_normal_(self.pos_embed, std=.02)
        if self.cls_token is not None:
            nn.init.normal_(self.cls_token, std=1e-6)
        named_apply(get_init_weights_vit(mode, head_bias), self)

    def _pos_embed(self, x):
        if self.no_embed_class:
            # deit-3, updated JAX (big vision)
            # position embedding does not overlap with class token, add then concat
            x = x + self.pos_embed
            if self.cls_token is not None:
                x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)
        else:
            # original timm, JAX, and deit vit impl
            # pos_embed has entry for class token, concat then add
            if self.cls_token is not None:
                x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)
            x = x + self.pos_embed
        return self.pos_drop(x)

    def forward_features(self, x):
        x = self.patch_embed(x)
        x = self._pos_embed(x)
        x = self.norm_pre(x)
        # if self.grad_checkpointing and not torch.jit.is_scripting():
        #     x = checkpoint_seq(self.blocks, x)
        # else:
        x = self.blocks(x)
        x = self.norm(x)
        return x

    def forward_head(self, x, pre_logits: bool = False):
        if self.global_pool:
            x = x[:, self.num_prefix_tokens:].mean(dim=1) if self.global_pool == 'avg' else x[:, 0]   #token
        x = self.fc_norm(x)
        return x if pre_logits else self.head(x)

    def forward(self, x):
        x = self.forward_features(x)
        x = self.forward_head(x)
        return torch.sigmoid(x)

    def get_attention_weights(self):
        return [block.get_attention_weights() for block in self.blocks]

def init_weights_vit_timm(module: nn.Module, name: str = ''):
    """ ViT weight initialization, original timm impl (for reproducibility) """
    if isinstance(module, nn.Linear):
        trunc_normal_(module.weight, std=.02)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif hasattr(module, 'init_weights'):
        module.init_weights()


def init_weights_vit_jax(module: nn.Module, name: str = '', head_bias: float = 0.):
    """ ViT weight initialization, matching JAX (Flax) impl """
    if isinstance(module, nn.Linear):
        if name.startswith('head'):
            nn.init.zeros_(module.weight)
            nn.init.constant_(module.bias, head_bias)
        else:
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.normal_(module.bias, std=1e-6) if 'mlp' in name else nn.init.zeros_(module.bias)
    elif isinstance(module, nn.Conv2d):
        lecun_normal_(module.weight)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif hasattr(module, 'init_weights'):
        module.init_weights()


def init_weights_vit_moco(module: nn.Module, name: str = ''):
    """ ViT weight initialization, matching moco-v3 impl minus fixed PatchEmbed """
    if isinstance(module, nn.Linear):
        if 'qkv' in name:
            # treat the weights of Q, K, V separately
            val = math.sqrt(6. / float(module.weight.shape[0] // 3 + module.weight.shape[1]))
            nn.init.uniform_(module.weight, -val, val)
        else:
            nn.init.xavier_uniform_(module.weight)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif hasattr(module, 'init_weights'):
        module.init_weights()


def get_init_weights_vit(mode='jax', head_bias: float = 0.):
    if 'jax' in mode:
        return partial(init_weights_vit_jax, head_bias=head_bias)
    elif 'moco' in mode:
        return init_weights_vit_moco
    else:
        return init_weights_vit_timm


class Superpixel_Vit(nn.Module):
    def __init__(self,in_feats_intra=1024,n_hidden_intra=1024,out_feats_intra=1024,in_feats_inter=1024,n_hidden_inter=1024,out_feats_inter=1024,vw_num=16,feat_dim=1024,num_classes=1,depth=1,num_heads = 16,final_fea_type = 'mean',mpool_intra='global_mean_pool',mpool_inter='global_mean_pool',gnn_intra='sage',gnn_inter='sage'):
        super(Superpixel_Vit, self).__init__()

        self.vw_num = vw_num
        self.feat_dim = feat_dim

        #intra-graph
        self.gcn1 = Intra_GCN(in_feats=in_feats_intra,n_hidden=n_hidden_intra,out_feats=out_feats_intra,mpool_method=mpool_intra,gnn_method=gnn_intra)

        #inter-graph
        self.gcn2 = Inter_GCN(in_feats=in_feats_inter,n_hidden=n_hidden_inter,out_feats=out_feats_inter,mpool_method=mpool_inter,gnn_method=gnn_inter)
        self.vit = VisionTransformer(num_patches = vw_num,num_classes = num_classes, embed_dim = feat_dim,depth = depth,num_heads = num_heads)

        self.final_fea_type = final_fea_type

    def superpixel_graph(self,data):
        superpixel_attri = data.superpixel_attri
        min_value = int(min(superpixel_attri))
        #intra-graph
        superpixel_fea = self.gcn1(data)
        #intra-graph
        superpixel_feas = self.gcn2(data,superpixel_fea)
        if min_value == 0:
            print('min superpixel value is 0')
            superpixel_fea_all={}
            for index in range(1,superpixel_feas.shape[0]):
                fea = superpixel_feas[index].unsqueeze(0)
                superpixel_value = index
                superpixel_fea_all[superpixel_value] = fea
        else:
            print('min superpixel value is 1')
            superpixel_fea_all={}
            for index in range(superpixel_feas.shape[0]):
                fea = superpixel_feas[index].unsqueeze(0)
                superpixel_value = index+1
                superpixel_fea_all[superpixel_value] = fea
        return superpixel_fea_all


    def mean_feature(self,superpixel_features,cluster_info):
        mask=np.zeros((self.vw_num,self.feat_dim))
        mask=torch.tensor(mask).to(device)
        # superpixel_cluster_path = os.path.join(cluster_info_path,slidename+'.pth')
        # cluster_info = torch.load(superpixel_cluster_path)
        for vw in range(self.vw_num):
            fea_all=torch.Tensor().to(device)
            for superpixel_value in cluster_info.keys(): 
                if cluster_info[superpixel_value]['cluster']==vw:
                    if fea_all.shape[0]==0:
                        fea_all=superpixel_features[superpixel_value]
                    else:
                        fea_all=torch.cat((fea_all,superpixel_features[superpixel_value]),dim=0)
            if fea_all.shape[0]!=0:
                fea_avg=torch.mean(fea_all,axis=0)
#             print('fea_avg shape:{}'.format(fea_avg.shape))
                mask[vw]=fea_avg
        return mask

    def max_feature(self,superpixel_features,cluster_info):
        mask=np.zeros((self.vw_num,self.feat_dim))
        mask=torch.tensor(mask).to(device)
        # superpixel_cluster_path = os.path.join(cluster_info_path,slidename+'.pth')
        # cluster_info = torch.load(superpixel_cluster_path)
        for vw in range(self.vw_num):
            fea_all=torch.Tensor().to(device)
            for superpixel_value in cluster_info.keys(): 
                if cluster_info[superpixel_value]['cluster']==vw:
                    if fea_all.shape[0]==0:
                        fea_all=superpixel_features[superpixel_value]
                    else:
                        fea_all=torch.cat((fea_all,superpixel_features[superpixel_value]),dim=0)
            if fea_all.shape[0]!=0:
                fea_max,_=torch.max(fea_all,dim=0)
#             print('fea_avg shape:{}'.format(fea_avg.shape))
                mask[vw]=fea_max
        return mask

    def forward(self,data,cluster_info):

        superpixels_fea = self.superpixel_graph(data)
        #final-fea
        if self.final_fea_type == 'mean':
            fea = self.mean_feature(superpixels_fea,cluster_info) #[16ï¼Œ1024]
        elif self.final_fea_type == 'max':
            fea = self.max_feature(superpixels_fea,cluster_info)
        fea = fea.unsqueeze(0)  #[1,16,1024]
        # print(fea.shape)
        # print(fea.shape)
        #vit
        fea = fea.float()
        out = self.vit(fea)
        return out

    def get_attention_weights(self):
        return self.vit.get_attention_weights()
